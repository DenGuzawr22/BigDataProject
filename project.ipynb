{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Project\n",
    "Developed by: \n",
    "- Grushchak Denys: mat. 0001027862\n",
    "- Bacca Riccardo: mat. 0001045029\n",
    "\n",
    "Original dataset (reduced to 7gb for the scope of this project): https://www.kaggle.com/datasets/dilwong/flightprices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\"executorMemory\":\"8G\", \"numExecutors\":2, \"executorCores\":3, \"conf\": {\"spark.dynamicAllocation.enabled\": \"false\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val bucketname =\"unibo-bd2223-rbacca\"\n",
    "\n",
    "val path_flights_db = \"s3a://\"+bucketname+\"/bigdata-project/xaa\"\n",
    "\n",
    "sc.applicationId\n",
    "\n",
    "\"SPARK UI: Enable forwarding of port 20888 and connect to http://localhost:20888/proxy/\" + sc.applicationId + \"/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Parsing\n",
    "This parser makes the original dataset flat, based on the leg's number of each flight.\n",
    "\n",
    "For example: if a flight has two legs it will be splitted into 2 rows.\n",
    "\n",
    "Each row, now has this properties:\n",
    "* duplicated fields: `id`, `flightDate`, `startingAirport`, `destinationAirport`, `isNonStop`, `isBasicEconomy`.\n",
    "* separated fields (based on original leg's number for this flight): `airplaneType`, `airlineName`, `segmentDistance`, `segmentDuration`.\n",
    "* recalculate `totalFare` for each row based on `segmentDistance`. Sum of two rows must return the original value of `totalFare`\n",
    "* each time-based value, will be converted in minutes\n",
    "\n",
    "`id` is a new numeric field that replace the orinal text identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type MyTuple = (String, String, String, String, String, Boolean, Boolean, Double, Int, Int)\n",
    "\n",
    "case class FlatFlightData(\n",
    "    id: Long,\n",
    "    flightDate: String,   \n",
    "    startingAirport: String,\n",
    "    destinationAirport: String,\n",
    "    airplaneType: String,\n",
    "    airlineName: String,\n",
    "    isNonStop: Boolean,\n",
    "    isBasicEconomy: Boolean,\n",
    "    totalFare: Double,\n",
    "    segmentDuration: Int,\n",
    "    segmentDistance: Int\n",
    ") extends Serializable{\n",
    "    // return a tuple of the object\n",
    "    def un() = FlatFlightData.unapply(this).get \n",
    "}\n",
    "\n",
    "\n",
    "object FlatParser{\n",
    "    import scala.collection.mutable.ListBuffer\n",
    "    \n",
    "    val commaRegex = \",(?=(?:[^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)\"\n",
    "    val doubleVerticalLine = \"\\\\|\\\\|\"\n",
    "    \n",
    "    def convertAirplaneType(value: String): Array[String] = {\n",
    "        //airplane type [String||String] \n",
    "        val result = value.split(doubleVerticalLine)\n",
    "        if (result.contains(\"\")) Array.empty\n",
    "        else result\n",
    "    }\n",
    "    \n",
    "    def convertAirlineName(value: String): Array[String] = {\n",
    "        // airliine name  [String||String]\n",
    "        val result = value.split(doubleVerticalLine)\n",
    "        if (result.contains(\"\")) Array.empty\n",
    "        else result\n",
    "    }\n",
    "    \n",
    "    //return array of duration of each leg in minutes\n",
    "    def convertSegmentsDuration(value: String): Array[Int] = {\n",
    "        val result = value.split(doubleVerticalLine)\n",
    "        if (result.contains(\"\")) Array.empty\n",
    "        else result.map(_.toInt/60)\n",
    "    }\n",
    "    \n",
    "    def convertSegmentsDistance(value: String): Array[Int] = {\n",
    "        // segment distance in miles: Vector of [Int||Int] can be [None||None]\n",
    "        val result = value.split(doubleVerticalLine)\n",
    "        if(result.contains(\"\") || result.head == \"None\") Array.empty\n",
    "        else result.map(_.toInt)\n",
    "    }\n",
    "   \n",
    "    def parseFlightInformationLine(line: String): Option[ListBuffer[MyTuple]] = {\n",
    "        try {\n",
    "            val input = line.split(commaRegex)\n",
    "            \n",
    "            val airplanes = convertAirplaneType(input(23))\n",
    "            val airline = convertAirlineName(input(21))\n",
    "            val segmentsDuration = convertSegmentsDuration(input(24))\n",
    "            val segmentsDistance = convertSegmentsDistance(input(25))\n",
    "            if (airplanes.isEmpty || \n",
    "                airline.isEmpty || \n",
    "                segmentsDuration.isEmpty || \n",
    "                segmentsDistance.isEmpty|| \n",
    "                airplanes.length != airline.length ||\n",
    "                airplanes.length != segmentsDuration.length ||\n",
    "                airplanes.length != segmentsDistance.length ) None\n",
    "            else {\n",
    "                val arr: ListBuffer[MyTuple] = ListBuffer()\n",
    "                val distanceSum = segmentsDistance.reduce(_+_).toDouble\n",
    "                for (i <- 0 until airplanes.length){\n",
    "                    val legPriceBasedOnDistance: Double = (segmentsDistance(i).toDouble/distanceSum) * input(12).toDouble //12 baseFare in $\n",
    "                    arr += ((input(2), //flightdate\n",
    "                             input(3), //starting airport\n",
    "                             input(4), //destination airport\n",
    "                             airplanes(i),\n",
    "                             airline(i),\n",
    "                             input(10).toBoolean,  //isNonStop\n",
    "                             input(8).toBoolean,  //isBasicEconomy\n",
    "                             legPriceBasedOnDistance,\n",
    "                             segmentsDuration(i),\n",
    "                             segmentsDistance(i))\n",
    "                    )\n",
    "                }\n",
    "                Some(arr)\n",
    "            }\n",
    "        } catch {\n",
    "            case _: Exception => None\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tolerance for duplicated data\n",
    "The original dataset has many duplicated record. \n",
    "We decided to create a new dataset that has the original format without duplicated records, then we use this new dataset as the new main source of data for elaboration.\n",
    "\n",
    "After elaboration of 10 GB of data we find out that only 1.2 GB of data were unique and this is not sufficient to comply project requirements, at this point we decided to continue using the original duplicated dataset even if it is practically not correct."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Purposes of this code:\n",
    "1. Read and parse the original data\n",
    "2. Create and append a new numeric identifier\n",
    "3. Insert the data in `FlatFlighData` class\n",
    "4. Create a cache\n",
    "\n",
    "We use `persist()` function two times because the function `zipWithIndex()` automatically trigger a spark job and as a conseguense before saving the final version of data in cache we need to parse twice the original data. The indermediate disk only cache save us from second parsing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.storage.StorageLevel._\n",
    "\n",
    "sc.getPersistentRDDs.foreach(_._2.unpersist()) \n",
    "\n",
    "val parsedData = sc.textFile(path_flights_db).flatMap(FlatParser.parseFlightInformationLine).persist(DISK_ONLY)\n",
    "val rddFlightsCached = parsedData.\n",
    "    zipWithIndex(). // Introduce numeric id for each flight row\n",
    "    map({case (v, i) => (i, v)}).\n",
    "    flatMapValues(m => m).\n",
    "    map({case (k, v) => FlatFlightData.apply(k, v._1, v._2, v._3, v._4, v._5, v._6, v._7, v._8, v._9, v._10)}).persist(MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "// To verify that parsing process was correct\n",
    "rddFlightsCached.map(_.un).take(20).foreach(println)\n",
    "\n",
    "val numberOfDistinctRecords = rddFlightsCached.map(_.id).distinct.count\n",
    "\n",
    "\"number of total (undistinct) records: \" + rddFlightsCached.count\n",
    "\n",
    "def round(v: Double): Double = {\n",
    "    (v*100).toInt/100.toDouble\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorative queries\n",
    "\n",
    "1. How many distinct airports and aircraft models\n",
    "2. Average travel duration for airline\n",
    "3. Percentage of basic economy tickets, based on all tickets\n",
    "4. Percentage of non-stop flights (flights with one leg)\n",
    "5. Average and price range of tickets\n",
    "6. Average ticket price for each airline\n",
    "7. Average and range of travel distance\n",
    "8. Top 10 airports with more arriving flights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "//1.How many distinct airports and aircraft models\n",
    "val distinctAirports = rddFlightsCached.\n",
    "    map(x => x.startingAirport).\n",
    "    distinct.\n",
    "    union(rddFlightsCached.map(x => x.destinationAirport).distinct).\n",
    "    distinct.count\n",
    "val distinctAircraftModels = rddFlightsCached.flatMap(x => x.airplaneType).distinct.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "//2. Average travel duration for each airline\n",
    "val distinctAirlines = rddFlightsCached.\n",
    "    map(x => (x.airlineName, x.segmentDuration)).\n",
    "    aggregateByKey((0.0, 0.0))((a,v)=>(a._1+v, a._2+1), (a1,a2)=>(a1._1+a2._1, a1._2+a2._2)).\n",
    "    map({case(k,v)=>(k,v._1/v._2)}).\n",
    "    collect().\n",
    "    foreach({case (airline, value) => println(airline + \" => \" + round(value) + \" avg minutes\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "//3. Percentage of basic economy tickets, based on all tickets\n",
    "\"Basic economy tickets: \" + round(\n",
    "    (rddFlightsCached.filter(_.isBasicEconomy).map(_.id).distinct.count.toDouble/numberOfDistinctRecords).toDouble*100) + \" %\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "//4. Percentage of non-stop flights\n",
    "\"Non-stop flights: \" + round(\n",
    "    (rddFlightsCached.filter(_.isNonStop).map(_.id).distinct.count.toDouble/numberOfDistinctRecords).toDouble*100) + \" %\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "//5. Average and price range of tickets\n",
    "val ticketPrices = rddFlightsCached.map(x => (x.id,x.totalFare)).reduceByKey(_+_).map(_._2)\n",
    "\"Range of prices: \"  + ticketPrices.min + \"$ to \" + ticketPrices.max + \"$\"\n",
    "\"Avg price: \" + round((ticketPrices.sum/numberOfDistinctRecords).toDouble) + \"$\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "//6. Average ticket price for each airline\n",
    "val avgTicketPricePerAirline = rddFlightsCached.\n",
    "    map(x => (x.airlineName,(x.totalFare, 1))).\n",
    "    reduceByKey((a,b) => (a._1+b._1, a._2+b._2)).\n",
    "    map(m => (m._1,m._2._1/m._2._2)).\n",
    "    collect.foreach({case (name, value) => println(name + \" => \" + round(value) + \"$\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "//7. Average and range of travel distance\n",
    "val travelDistances = rddFlightsCached.map(x => (x.id, x.segmentDistance)).reduceByKey(_+_).map(_._2)\n",
    "\"Range of distances: \"  + travelDistances.min + \" to \" + travelDistances.max + \" miles\"\n",
    "\"Avg travel distance: \" + round((travelDistances.sum/numberOfDistinctRecords).toDouble) + \" miles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "//8. Top 10 airports with more arriving flights\n",
    "val topAirports = rddFlightsCached.\n",
    "    map(x => (x.id, x.destinationAirport)).\n",
    "    distinct().\n",
    "    map(m => (m._2, 1)).\n",
    "    reduceByKey(_+_).    \n",
    "    sortBy(_._2, false).\n",
    "    take(10).\n",
    "    foreach({case (name, value) => println(name + \" => \" + value)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val path_output = \"s3a://\"+bucketname+\"/spark/avgRatPerMovie\" //todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Riccardo\n",
    "\n",
    "> Aggrego su “aircraft models” per calcolare la classifica discreta dei modelli più usati rispetto a ciascuna “airline”. \n",
    "Eseguo il join col dataset originale, infine riaggrego su “travel duration” e sulla classificazione di prima. \n",
    "Ottengo la durata media di ogni volo per ogni compagnia (oltre a numero di voli e totale di ore) sul modello di aereo più utilizzato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "// Print results\n",
    "def printQueryResult(result: ((String, String),(Int, Double, Double))): Unit = {\n",
    "    result match {\n",
    "        case ((compagnia, aereo),(numeroVoli, totOre, mediaVolo)) => \n",
    "            println(compagnia + \" ====> \" + aereo + \" ====> \" + \"numero voli: \" + numeroVoli + \" - totale ore: \" + totOre + \" - media in volo (ore): \" + mediaVolo)\n",
    "    }\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "//primo caso: prima riduco tutti i dati, e poi faccio join (join più piccolo)\n",
    "\n",
    "val mostUsedAircraftForAirline = rddFlightsCached.map(x => ((x.airlineName,x.airplaneType), 1)).\n",
    "        reduceByKey(_+_).\n",
    "        map({case ((airlineName, airplaneType),count) => (airlineName, (airplaneType,count))}).\n",
    "        reduceByKey((a,b) => if (a._2 > b._2) a else b).\n",
    "        map({case (airlineName, (airplaneType, count)) => ((airlineName, airplaneType),count)})\n",
    "\n",
    "val totalFlightDurationForAirlineAirplane = rddFlightsCached.map(x => ((x.airlineName, x.airplaneType),x.segmentDuration)).reduceByKey(_+_)\n",
    "\n",
    "val result = mostUsedAircraftForAirline.join(totalFlightDurationForAirlineAirplane).\n",
    "    map({case (k, (count, sum)) => (k, (count, round(sum.toDouble/60), round((sum.toDouble/count)/60)))}).\n",
    "    collect().\n",
    "    foreach(printQueryResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "//secondo caso: join più grande, meno dati da mappare e calcolo la somma delle durate dopo\n",
    "\n",
    "val mostUsedAircraftForAirline = rddFlightsCached.map(x => ((x.airlineName,x.airplaneType), 1)).\n",
    "        reduceByKey(_+_).\n",
    "        map({case ((airlineName, airplaneType),count) => (airlineName, (airplaneType,count))}).\n",
    "        reduceByKey((a,b) => if (a._2 > b._2) a else b).\n",
    "        map({case (airlineName, (airplaneType, count)) => ((airlineName, airplaneType),count)})\n",
    "\n",
    "val totalFlightDurationForAirlineAirplane = rddFlightsCached.map(x => ((x.airlineName, x.airplaneType),x.segmentDuration))\n",
    "\n",
    "val result = mostUsedAircraftForAirline.join(totalFlightDurationForAirlineAirplane).reduceByKey((a,b) => (a._1, (a._2 + b._2))).\n",
    "    map({case (k, (count, sum)) => (k, (count, round(sum.toDouble/60), round((sum.toDouble/count)/60)))}).\n",
    "    collect().\n",
    "    foreach(printQueryResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "// terzo caso: uso aggregate\n",
    "val mostUsedAircraftForAirline = rddFlightsCached.map(x => ((x.airlineName,x.airplaneType), x.segmentDuration)).\n",
    "        aggregateByKey((0.0, 0.0))((a,v)=>(a._1+v, a._2+1), (a1,a2)=>(a1._1+a2._1, a1._2+a2._2)).\n",
    "        map({case ((airline, airplane), (sum, count)) => (airline, (airplane, sum, count))}).\n",
    "        reduceByKey((a,b) => if (a._3 > b._3) a else b)\n",
    "\n",
    "val result = mostUsedAircraftForAirline.\n",
    "    map({case (airline, (airplane, sum, count)) => ((airline, airplane), (count.toInt, round(sum.toDouble/60), round((sum.toDouble/count)/60)))}).\n",
    "    collect().\n",
    "    foreach(printQueryResult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-26T17:47:01.300868Z",
     "iopub.status.busy": "2023-01-26T17:47:01.300635Z",
     "iopub.status.idle": "2023-01-26T17:47:01.561303Z",
     "shell.execute_reply": "2023-01-26T17:47:01.560507Z",
     "shell.execute_reply.started": "2023-01-26T17:47:01.300844Z"
    },
    "tags": []
   },
   "source": [
    "### Denys\n",
    "I will calculate the velocity of each airplane type\n",
    "> Denys: aggrego su “airplane model” per calcolare la “segment distance” totale percorsa da ogni modello, poi faccio self-join e aggregazione per determinare il “travel duration” per ogni “airplane model”. Alla fine determino la velocità media di ogni modello partendo dai dati aggregati. Visualizzo 10 aerei con la velocità media più alta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "//return velocity in miles per hour\n",
    "def getVelocity(distance: Int, durationInMinutes: Int): Double = {\n",
    "    round(distance.toDouble/(durationInMinutes.toDouble/60))\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T18:28:01.319109Z",
     "iopub.status.busy": "2023-01-30T18:28:01.318837Z",
     "iopub.status.idle": "2023-01-30T18:28:01.587699Z",
     "shell.execute_reply": "2023-01-30T18:28:01.586817Z",
     "shell.execute_reply.started": "2023-01-30T18:28:01.319080Z"
    },
    "tags": []
   },
   "source": [
    "First solution follows the problem description: there are two distinct agregation for distance and duration and then a self join the unite aggregated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val totalAirplanesDistance = rddFlightsCached.map(m => (m.airplaneType, m.segmentDistance)).reduceByKey(_+_)\n",
    "val totalAirplanesDuration = rddFlightsCached.map(m => (m.airplaneType, m.segmentDuration)).reduceByKey(_+_)\n",
    "\n",
    "totalAirplanesDistance.join(totalAirplanesDuration).\n",
    "    map({case (name, (dis, dur)) =>  (name, getVelocity(dis, dur))}).\n",
    "    sortBy(_._2,false).\n",
    "    take(10).\n",
    "    foreach({case (a, v) => println(a + \" ----> \" + v + \" mph\")})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In next implementation I tried to make a repartitioning of the data on base of common key but it gives a worst result because repartioning overhead is much bigger than the quantity of data that are suffle in the first query. It is because the first query aggregatetion produces a very few lines of result that can be shuffle very fast while the repartitioning shuffle all selected data.\n",
    "\n",
    "For example: for 7 GB dataset the first query have only ~1Mb of suffle reads and writes while this solution have ~460MB of shuffle writes and ~920MB of shuffle reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val repartitionedValues = rddFlightsCached.map(m => (m.airplaneType, (m.segmentDistance, m.segmentDuration))).repartition(12)\n",
    "\n",
    "val totalAirplanesDistance = repartitionedValues.map(m => (m._1, m._2._1)).reduceByKey(_+_)\n",
    "val totalAirplanesDuration = repartitionedValues.map(m => (m._1, m._2._2)).reduceByKey(_+_)\n",
    "\n",
    "totalAirplanesDistance.join(totalAirplanesDuration).\n",
    "    map({case (name, (dis, dur)) =>  (name, getVelocity(dis, dur))}).\n",
    "    sortBy(_._2,false).\n",
    "    take(10).\n",
    "    foreach({case (a, v) => println(a + \" ----> \" + v + \" mph\")})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fastes way for the velocity calculation is aggregation of two fields together. \n",
    "\n",
    "The performance is obtained by aggregating the data only once and due to absence of the join operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rddFlightsCached.\n",
    "    map(m => (m.airplaneType, (m.segmentDistance, m.segmentDuration))).\n",
    "    reduceByKey({case ((dis1, dur1), (dis2, dur2)) => (dis1+dis2, dur1+dur2)}).\n",
    "    map({case (name, (dis, dur)) =>  (name, getVelocity(dis, dur))}).\n",
    "    sortBy(_._2, false).\n",
    "    take(10).\n",
    "    foreach({case (a, v) => println(a + \" ----> \" + v + \" mph\")})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative velocity valutation (mean of means): first we calculate the mean velocity of each flight and then we calculate the mean velocity for each airplane type. This query structure is very similar to the previous one so they have very similar performance. This type of velocity valutation is impossible to do with seperate vale aggregation as in first two queries.\n",
    "\n",
    "Obviously the query result is different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rddFlightsCached.\n",
    "    map(m => (m.airplaneType, (m.segmentDistance.toDouble, m.segmentDuration.toDouble/60))).\n",
    "    aggregateByKey((0.0,0.0))((a,v)=>(a._1+(v._1/v._2), a._2+1),(a1,a2)=>(a1._1+a2._1, a1._2+a2._2)).\n",
    "    map({case (name, (velocitySum, count)) =>  (name, round(velocitySum/count))}).\n",
    "    sortBy(_._2, false).\n",
    "    take(10).\n",
    "    foreach({case (a, v) => println(a + \" ----> \" + v + \" mph\")})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "scala",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
